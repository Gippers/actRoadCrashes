---
title: "Assignment 2 - GLM - Question 1"
author: "Group X"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  pdf:
      #- footskip=33pt
    toc: false
    fig-cap-location: top # Make sure the fig is at top, not working?
    number-sections: true
    number-depth: 2
    #number-offset: 2
    colorlinks: true
    html-table-processing: none
    fontsize: "12"
    mainfont: "Times New Roman"
    linestretch: 1
    fig-pos: "H"
    geometry:
      - inner=1cm
      - outer=1cm
      - top=0.5cm
      - bottom= 3cm
      - headsep=10pt
      - headheight=11pt
      - ignorehead
      - ignorefoot
      - heightrounded
execute:
  echo: false
  warning: false
#header-includes:
  #\usepackage{float}
  #\floatplacement{table}{H}
editor: 
  markdown: 
    wrap: 72
---

```{r}
# Load necessary libraries
library(tidyverse)
library(MASS) # Ordered logistic regression
library(here)
library(data.table)
library(astsa)
library(TSA)
library(forecast)
library(lubridate)
library(readxl)
library(patchwork)
library(sandwich) # Account for seial autocorrelation in errors
```

# Introduction

The data used for this report was collected by the AFP Crash Report Form, and made public by on the ACT Open Data Portal as at 30 April 2025 [1]. All crashes in the ACT must be reported to police within 24 hours, unless attended by police [3]. This includes all road users, drivers, cyclists, pedestrians and motorcyclists. This additionally includes crashes on paths and other road related areas involving pedestrians or cyclists. For this analysis, the only relevant information is the date. All crash incidents are aggregated by date, and transformed into a time series for time series modelling. (TODO this last bit is a little clunky). Therefore, the time series measures daily road crashes in the ACT reported through the AFP crash report system, hereafter referred to as ACT road crashes. This reported limited the timeline of the data from 2015 to 2019 in order to model the data without the presence of the COVID lockdowns skewing the result (TODO decide what to do with covid period). 

# Data Characteristics


We will refer to the number of car crashes as $y_t$, where $t = 1, 2, ..., 1825, 1826$.
```{r}
# Load the crash data and aggregate by day
crash <- read.csv(paste0(here::here(), "/ACT_Road_Crash_data_20250430.csv"))
setDT(crash)
# Aggregate count of crashes by day, note each obsevation does have a unique crashID
crash <- crash[, .(NUM_CRASHES = .N), by = .(CRASH_DATE)] 
crash[, time := 1:.N]

crash[, DATE := as.Date(CRASH_DATE, format = "%d/%m/%Y")] # Set date
crash[, CRASH_DATE := NULL]

# Keep only dates until end of 2019
# TODO should I include covid?
crash <- crash[DATE <= as.Date("31/12/2019", format = "%d/%m/%Y")]

# TODO turn into a time series
ts.crash <- ts(crash$NUM_CRASH)

#####################
# Add in the public holiday etc. data
###########
# Create day variables
crash$day <- factor(lubridate::wday(as.Date(crash$DATE), label = T),
                       ordered = F)
crash$month <- factor(lubridate::month(as.Date(crash$DATE), label = T),
                       ordered = F)

# Load the public holiday data
public_holidays <- readxl::read_excel(paste0(here::here(), 
                                           "/public_school_holidays.xlsx"),
                                    sheet = "public_holidays")
school_holidays <-  read_excel("public_school_holidays.xlsx", sheet = "school_holidays")
setDT(public_holidays)
setDT(school_holidays)
public_holidays[, rn := NULL]
# Fix the date variables
colnames(public_holidays)[1] <- "DATE"
public_holidays[, DATE := as.Date(DATE, format = "%A %d %B %Y")][
                , ph := 1
]
colnames(school_holidays)[1] <- "DATE"
school_holidays[, DATE := as.Date(DATE, "%d/%m/%Y")]

# Merge the holiday data onto the crash dataset
crash <- merge.data.table(crash, public_holidays, by = c("DATE"), all.x = T, all.y = F)
crash <- crash[is.na(ph), ph := 0]
crash <- merge.data.table(crash, school_holidays, by = c("DATE"), all.x = T, all.y = F)
crash <- crash[is.na(school_holiday), school_holiday := 0]

# Now create holidays variable that considers normal day, school_holiday, public holiday 
# or both school and public holiday
crash[, holiday := "None"]
crash <- crash[ph == 1 & school_holiday == 1, holiday := "Public and School Holiday"]
crash <- crash[ph == 0 & school_holiday == 1, holiday := "School Holiday"]
crash <- crash[ph == 1 & school_holiday == 0, holiday := "Public Holiday"]
crash[, holiday := as.factor(holiday)]

# Slightly different different day encoding:
crash$day2 <- "Sun"
crash <- crash[day == "Mon", day2 := "Mon"][
                day %in% c("Tue", "Wed"), day2 := "Tue-Wed"][
                    day %in% c("Thu", "Fri"), day2 := "Thu-Fri"][
                        day == "Sat", day2 := "Sat"]
crash <- crash[, day2 := as.factor(day2)]

# simpler month encoding
crash$month2 <- as.character(crash$month)
crash <- crash[month %in% c("Apr", "May"), month2 := "Apr-May"][
                month %in% c("Aug", "Sep", "Oct"), month2 := "Aug-Oct"]
crash <- crash[, month2 := as.factor(month2)]
crash$month2 <- relevel(crash$month2, "Jan")

# Creating the 50/50 train test split dataset
t <- 912
t.prime <- 1825

crash.train <- crash[2:t]
crash.test <- crash[(t + 1):t.prime]
```



```{R}
# Histogram of the number of crashes
#hist(crash$NUM_CRASHES)
ggplot(crash, aes(x = NUM_CRASHES)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Total Daily Road Crashes in ACT", x = "Number of ACT Road Crashes", y = "Frequency") +
  theme_minimal()
```

```{R}
#| label: fig-tsplot
#| figfig-cap: "Daily Number of Reported ACT Road Crashes 2015 to 2019"
# Plot the entire time series
# Create labels for the first day of jan and july
# TODO work a little more on the formatting here
dates <- format(crash$DATE, "%d-%m") %in% c("01-01", "01-07")
date.labels <- crash$DATE[dates]
data.axis <- which(dates)
plot(ts.crash, xaxt = "n", ylab = "Number of Crashes", bty = "L", xlab = "Date", 
     main = "ACT Road Accidents by Day")
axis(1, data.axis, format(date.labels, "%m/%y"), las = 2)
lines(lowess(ts.crash), col = 'red', lwd = 2)
legend("topright", legend = c("LOWESS", "Train/Test Split"), 
       col = c("red", "blue"), lwd = 2, lty = c(1, 2),
       bty = "n")
# add a vertcal line for the train test split
abline(v = 912, col = "blue", lwd = 2, lty = 2)

```
We can see a number of things happening here, it is clear that the mean of the time series depends on time, with overall a slight decrease in the second half of the time series. This creates a challenge as the model will be trained on the first half of the data set only which appears to in fact have a stationary mean, this is unavoidable though as the test data set is treated as un observed for the pruposes of the modelling. Therefore, from here on out the EDA will focus on the $1 \ldots T$ data points as this will guide later model selection. There is also a very slight decrease in variance over time, and at certain periods in the dataset such as around January in year. In addition, there is evidence of seasonality, once again around January there is a drop in the number of road crash reports and when you look at a more mirco level there is some quite obvious weekly seasonality. 


The first goal of this analysis is to make the data stationary, in order to apply Autoregressive Moving Average (ARMA) models to the data. This requires the stabilisation of the mean, variance and the elimination of seasonality in the data. In order to stabilize the variance a boxplot transformation will be applied and, the data first difference will be taken in order to stabilise the mean. 

```{r}
# Find the value of lambda
lambda <- round(BoxCox.lambda(crash.train$NUM_CRASHES), digits = 4)
```
Using the BoxCox procedure, we find that $\lambda = `r lambda`$, therefore to stabilise the variance we will apply a square root transform to $y_t$. Therefore, the stabilized and transfromed will be referred to as $z_t = y_t^{\frac{1}{2}} - 1$ and then the first difference data is, $z_t' = y_t^{\frac{1}{2}} - y_{t-1}^{\frac{1}{2}}$, where $t = 1, 2, ..., 1825, 1825$ [7]. Note the variance is stabilized before taking the first difference, as suggested by Hyndaman in Forecasting: Principles and Practice chapter 9 [8].

```{r}
#| label: fig-boxcox-d1
#| figfig-cap: "Square Root Transformed and First Differenced Daily Number of Reported ACT Road Crashes 2015 to 2019"


# Make sure crash is ordered correctly
crash <- crash[order(DATE)]
crash[, z := sqrt(NUM_CRASHES) - 1]
crash[, z_diff := z - shift(z, type = "lag")] # Just in case we need it

# Add z to crash and 
crash.train <- crash[2:t]
crash.test <- crash[(t + 1):t.prime]

# Box cox transform the data
bc.crash <- ts(sqrt(crash.train$NUM_CRASHES) - 1) # TODO check the order of this
d1.crash <- ts(diff(crash.train$NUM_CRASHES))

# Plot the entire time series
# Create labels for the first day of jan and july
# TODO work a little more on the formatting here
#diff.dates <- crash$DATE[-1]
dates <- format(crash$DATE, "%d-%m") %in% c("01-01", "01-07")
date.labels <- crash$DATE[dates]
data.axis <- which(dates)
plot(bc.crash, xaxt = "n", ylab = "Zt")
# TODO fix the start date as the second
axis(1, data.axis, format(date.labels, "%m/%y"), las = 2)
lines(lowess(bc.crash), col = 'red', lwd = 2)
legend("topright", legend = c("LOWESS"), 
       col = "red", lwd = 2)

acf(bc.crash)
pacf(bc.crash)
```
This procedure appears to have successfully stabilised variance of the time series. The mean has perhaps the slightest hint of an increase however, it appears to overall be stationary. A test will be conducted to see if there is a linear trend.However, it appears that there is definitely some seasonality. Let's explore this below:
- Note we also considered a 7 day and a 364 day difference however, the 7 day still had issues with monthly seasonality and the 364 day has issue as it is not date adjusted.

We can observe that the fitted model will most likely be described by both an autoregressive and a moving average component. The MA component quickly decay with the, which is indicative of a maybe a weekly seasonality (TODO need to check this). Also we can see that the most important partial autocorrelations appear to be from the first 6 components.



## Day of Week and Month of Year
My prior going into this analysis, is that road crashes will strongly correlated to days where there is the most volume of driving. This may run slightly contrary to the gut reaction which is that public holidays and long weekends have more accidents however, it is likely that there are more fatal accidents from long car drives rather than road crashes which is what we are attempting to measure. In the ACT, I hypothesise that workdays are the busiest days on the road as workers are commuting to work, businesses are recieving and sending deliveries and parents are taking their children to school (TODO get a stat for this). Therefore, I would expect that there depending on the day of the week, school holidays, public holidays, when workers tend to take leave and potentially the month of the year we should see some seasonality in the number of crashes. 



```{R}
# Plot the day, month and public holiday data fpr transformed data
g1 <- ggplot(crash.train, aes(x = day, y = z)) +
  geom_boxplot() +
  ggtitle("ACT Car Crashes by Day of Week")
g2 <- ggplot(crash.train, aes(x = month, y = z)) +
  geom_boxplot() +
  ggtitle("ACT Car Crashes by Month")
g3 <- ggplot(crash.train, aes(x = holiday, y = z)) +
  geom_boxplot() +
  ggtitle("ACT Car Crashes by Holiday")
combined <- g1 + g2 +g3
combined
```
As hypothesised, there is a strong pattern in the weekday data. It actually appears to be more cyclical than I expected, and it is possible that a sin or cosine function will fit this pattern adequately. The month data is a little more varied, there may be a repeating cyclical pattern however, I wonder whether this pattern could be related to the number of weekends in a month (which is non-constant) rather than an underlying cyclical monthly pattern. Definitely January and to a lesser extent December stand out as having significantly lower number of accidents, which is probably a product of the number of public holidays, leave and school holidays. Whilst there is not readily available data on leave habits, the past ACT school holidays and public holidays are available data and likely even serve as a proxy for worker leave periods.

Note, for the public holiday data, where they fall on a weekend only the weekday version is counted as it is hypothesised that this is the date that will affect traffic and as a result change the likelihood of road crashes.

# Model Fitting and Interpretation

```{r}
# Fit a time model z on the transformed data
time.model.test <- lm(z ~ time + day + month + holiday, 
                  data = crash.train)



z.model <- lm(z ~ day2 + month2 + holiday, 
                  data = crash.train)

# Summary that accounts for serial autocorrelation
# pvalues using a z-test
tt.coef <- z.model$coefficients
se <- sqrt(diag(vcovHAC(z.model)))
pv.tt <- pnorm(abs(tt.coef / se), lower.tail = F) * 2
tt.table <- round(data.frame(estimate = tt.coef, 
                                se = se, p.values = pv.tt), digits = 4)
tt.table

# Use anova to test if grouping is ok
an.test <- anova(z.model, time.model.test)
pv <- round(an.test$`Pr(>F)`, digits = 4)

BoxCox.lambda(z.model$residuals + 10)

# residuals for the model z 
tsdisplay(crash.train$z.r, ci.type = "ma")

#plot(z.model)
```
- This has procedure has done a good job of removing the trend, the seasonality and stabilising the variance. There is no clear sesonality, trend or levels shifts so now we can go ahead and fit a 
- TODO do I need to and how do I show this.

We fit the following linear regression model: (TODO need to update his)
$$z_t' = \beta_0 + \sum_{j = 1}^{6}\beta_j \text{day of week}_j + \beta_7 \text{public holiday} + \epsilon$$
Initially, we explore a model with time and every day of the week and that time is marginall insignificant and that it appears that we might be able to reduce every day of the week to Sun, Mon, Tue-Wed, Thu-Fri and Sat. We then go ahead and check these are indeed insignficant using a joint anove test, in which we fail to reject the null an d find no evidence to suggest the larger model fits better.(join f-test has p values `r pv`).

Discuss that I tried to approaches, a time poisson model and the box cox + difference plus linear model, but the poisson model didn't stabilise variance and the difference model overfit with a time trend. Keep in mind, the residuals are slightly a heavy heavy, which is likely a consquence of modelling the count data as a transformed linear model. Worked better than the poisson model however.



```{r}
tsdisplay(crash.train$z.r, ci.type = "ma")
```
Looks like we are just left with an MA(1), as the ACF has a spike at 2 and the PACF decays quickly. Lets fit some different models.

- note, may need to move some of the above model fitting into here, include only the stuff that analyses the dataset
TODO talk about how we are splitting the data
- how does diff play into the train test split? 
- talk about what t is here


## SEACF Model Fitting
```{r}
# SEACF for the time-trend residuals
eacf(crash.train$z.r, ar.max = 14, ma.max = 20)
```
There appears to be a clear choice of ARMA(0, 0, 1), which supports our analysis from the acf and pacf plots which showed a MA(1) process with a signicant lag at ACF of 1.
- TODO talk about using ML
```{r}
# Fit an Arima model
ARMA.SEACF.ML = Arima(crash.train$z.r, order=c(0, 0, 1),
                 method='ML', include.mean = F) # Don't include mean as alread in time trend model
#ARMA.SEACF.ML$code
# pvalues using a z-test
se <- sqrt(diag(ARMA.SEACF.ML$var.coef))
pv.ml <- (pnorm(abs(ARMA.SEACF.ML$coef / se), lower.tail = F)) * 2
seacf.table <- data.frame(estimate = ARMA.SEACF.ML$coef, 
                                se = se, p.values = pv.ml)
seacf.table
#ARMA.SEACF.CSS = Arima(train.bc.crash, order=c(4,0,5),
#                 method='CSS')
```
- The model converged
- Note we don't include the mean here as it is already included in the time-trend model

## AIC Model Selection
Fit an ARMA model predicting $z'$, using AIC to find the best model.
```{r}
# Fit the model using AIC
# Do at least 7 given the observed weekly seasonality
max.p <- 7
max.q <- 7
mod_aics <- matrix(0, max.p + 1, max.q + 1)
for(p in 0:max.p)
{
    for(q in 0:max.q)
    {
        aic <- Arima(crash.train$z.r, order = c(p, 0, q), method = "ML", include.mean = F)$aic
        mod_aics[p + 1, q + 1] <- aic
    }
}
mod_aics #- min(mod_aics)
```


We find that the ARMA(1, 0) model is the best fit using AIC
```{r}
# Fit the ARMA(1, 0 model
ARMA.AIC.ML = Arima(crash.train$z.r, order=c(1, 0, 0),
                 method='ML', include.mean = F)
#ARMA.AIC.ML$code
# pvalues using a z-test
se.aic <- sqrt(diag(ARMA.AIC.ML$var.coef))
pv.ml.aic <- (pnorm(abs(ARMA.AIC.ML$coef / se.aic), lower.tail = F)) * 2
aic.table <- data.frame(estimate = ARMA.AIC.ML$coef, 
                                se = se.aic, p.values = pv.ml.aic)
round(aic.table, digits = 4)
```
- TODO write out the formula
- TODO note that the mean is excluded because it is included in the time-trend model
- THe model converged


## SARIMA Model
- TODO build a SARIMA model from the transformed data only
- Analysis here based on chapter 8 of og forecastying principles and practice (look at my phone)
- 
```{r}
# Regular display of time series
tsdisplay(crash.train$z, ci.type = "ma", main = "Square Root Transformed ACT Road Crashes (Zt)")
```
- The ACF shows spikes in at lag 7, this indicates a seasonality of 7 (TODO maybe ref this from EDA, move this to EDA, still have this in code but only for train)

```{r}
# 7 differenced:

tsdisplay(diff(crash.train$z, lag = 7), ci.type = "ma", main = "Weekly Differced Square Root Transformed ACT Road Crashes (Zt - Z_{t-7})")
```
- Here we see a spike in the ACF at lag 7 but not too many other significant spikes (other than some early ones)
- The spikes in the PACF at lag 7 decay to 0
- Doesn't appear to be any instability, therefore we can consider $d = D = 0$. 
- Definitely positive non seaonal terms of p = 1, q = 1
- This adds up to a $ARIMA(1, 0, 1)(0, 0, 1)_7$ model. 
```{r}
# Try SARIMA(1, 0, 1)(0, 0, 1) and then other SARIMA's
sarima.101 <- Arima(crash.train$z, order = c(1, 0, 1), 
                seasonal = list(order = c(0, 0, 1), period = 7), method = "ML", )
tsdisplay(residuals(sarima.101), ci.type = "ma")
sarima.101.101 <- Arima(crash.train$z, order = c(1, 0, 1), 
                seasonal = list(order = c(1, 0, 1), period = 7), method = "ML", )
tsdisplay(residuals(sarima.101.101), ci.type = "ma")
sarima.201.101 <- Arima(crash.train$z, order = c(2, 0, 1), 
                seasonal = list(order = c(1, 0, 1), period = 7), method = "ML", )
sarima.final <- sarima.201.101
tsdisplay(residuals(sarima.final), ci.type = "ma", main = "Residuals of SARIMA(2,0,1)(1, 0,1)[7]")
```
- We try $ARIMA(1, 0, 1)(0, 0, 1)_7$, aic = 2275.24
- Still appears to be a lag in the seasonal component, try ARIMA(1, 0, 1)(1, 0, 1)_7$, aic = 1996.15
- improvement, still small evidence of a slight lag, add another 1 to p (need to explain this better, still small thing at 1 so I add one more lag to elim it) $SARIMA(2, 0, 1)(1, 0, 1)_7$, aic = 1990.92
- Finally, is there any improvement with d = 1, maybe a slight hint of decline: $SARIMA(2, 0, 1)(1, 0, 1)_7$ aic = 2005.45 $
Therefore, we decind to us a $SARIMA(2, 0, 1)(1, 0, 1)_7$.

```{r}
# Table for the samrim
# pvalues using a z-test
se.s <- sqrt(diag(sarima.final$var.coef))
pv.s <- (pnorm(abs(sarima.final$coef / se.s), lower.tail = F)) * 2
s.table <- data.frame(estimate = sarima.final$coef, 
                                se = se.s, p.values = pv.s)
round(s.table, digits = 4)
```
- TODO put the full formula here


## auto.arima with $z_t$ directly
The auto arima function defined by Hyndamyn using the time trend model suggest the same thing that the AIC selection suggests. Therefore, lets run auto.arima on teh non-stabilised $z_t$ to see how it performs. Because of apparent strong weekly seasonality, we will let the auto.arima find a model with weekly seasonality. We also turn off 
```{r}
# Fit a SARIMA model
a.sarima <- auto.arima(ts(crash.train$z, frequency = 7), max.p = 7, max.q = 7, max.order = 7, trace = T, seasonal = T)

# pvalues using a z-test
se.aa <- sqrt(diag(a.sarima$var.coef))
pv.aa <- (pnorm(abs(a.sarima$coef / se.aa), lower.tail = F)) * 2
as.table <- data.frame(estimate = a.sarima$coef, 
                                se = se.aa, p.values = pv.aa)
round(as.table, digits = 4)

tsdisplay(residuals(a.sarima), ci.type = "ma", main = "Residuals of SARIMA(1,0,0)(2, 0, 0)[7]")
```
- The Auto.Arima function has selected an SARIMA(1, 0, 0)(2, 0, 0)_7 model as the best fit. 
todo write out the formula here
- TODO interpret


# Model Diagnostics
## SEACF Model Diagnostics
```{R}
# Diagnostic plots for models

model_diagnostics <- function(model)
{
    # Given ar ARMA model produces model diagnostic plots
    # standardized resiuals, ACF and pvalues
    tsdiag(model, gof.lag = 21)
    
    # Get residuals information
    train_additional_info <- crash.train %>%
        mutate(residuals = model$residuals) %>%
        mutate(fitted_values = model$fitted)
    
    # QQplot
    ggplot(train_additional_info, aes(sample = residuals)) +
        geom_qq() +
        geom_qq_line() +
        theme_bw()
}



```

```{r}
# SEACF ARMA Model
model_diagnostics(ARMA.SEACF.ML)

# Get the outliers
res.stand <- rstandard(ARMA.SEACF.ML)
toutlier <- crash.train[abs(res.stand) > 3]$DATE # Time of the outlier

# analyse the outliers
crash.train[DATE %in% toutlier]
```
- Examining the model diagnostics, we find the the residuals do appear scattered around 0 with somewhat consistent variance around 0. Furthermore the variance assumption is met decently until the top top part of the distribution which is a little heavy tailed. Given the amount of data though we shouldn't be too concerned as the CLT will help ensure that a slight variation of this isn't too problematic, and for this analysis we don't use the variance. Finally, the Ljung-Box test fails. This means that there is still some serial autocorrelation in the data that we haven't accounted for. We will continue to use this model instead of iteratively trying to refit it to demonstrate the difference to the time-trend ARMA model selected via the AIC method, which has a different form.
- There does appear to be one outlier on the day Wednesday 2017-04-26, on this date there were school and public holidays on Monday and Tuesday and Wednesday would have been the first day back at school. This likely caused a higher than expected number of road accidents as the day following a weekend tends to have the highest accident rate. However, given that it looks like a one off, and you cannot remove time series outliers, I will not attempt to control for this point. Note that both time-trend based models have this outlier

## Time Trend model with AIC ARMA Residuals
```{r}
# Time Trend ARMA model
model_diagnostics(ARMA.AIC.ML)
# Get the outliers
res.stand <- rstandard(ARMA.AIC.ML)
toutlier <- crash.train[abs(res.stand) > 3]$DATE # Time of the outlier

# analyse the outliers
crash.train[DATE %in% toutlier]
```
- The constant variance, linearity of residuals assumptions are met for this model. Similarly to the previous model, the residuals are mostly normal with a slightly heavy right tail. 
- This time however, changing from ARMA(0, 0, 1) to a ARMA(2, 0, 2) in addition to improving the AIC also changed the residuals to be not correlated over time. Therefore, the residuals of this model is stationary and overall the assumptions of the model do not appear to be violated.
- It appears that EACF was inccorect, and that the AR(1) was a better fit to the residuals

## SARIMA Model
```{R}
# Auto.arima model
model_diagnostics(sarima.final)

# Get the outliers
res.stand <- rstandard(sarima.final)
toutlier <- crash.train[abs(res.stand) > 3]$DATE # Time of the outlier

# analyse the outliers
crash.train[DATE %in% toutlier]
```
- No issue with the constant variance or linearity of residuals
- Slightly heavy bottom tail, overall residuals are normal
- No major issues with the autocorrelation via box lejung

## Auto SARIMA Model
```{r}
# Auto.arima sarima model
model_diagnostics(a.sarima)

# Get the outliers
res.stand <- rstandard(a.sarima)
toutlier <- crash.train[abs(res.stand) > 3]$DATE # Time of the outlier

# analyse the outliers
crash.train[DATE %in% toutlier]
```
- No issue with the constant variance or linearity of residuals
- Residuals look very normal, some tiny amount of light tails but this is neglible
- Fails the serial autocorrelation test. Not accounting for the seasonality has left this model with serial autocorrelation at lag 7. 
A naive approach with auto.arima isn't the way to go.



# Data Prediction
In addition to the four models fitted and diagnosed, I will also include a Naive estimate. This is just the last value from the training time series at time $T$. Ideally, a good model would have a smaller prediction than this naive baseline, it is a good tool for comparison.
- Note that the time trend models predict from the time trend model plus add the errors



```{r}
##
# 1 step truncated predictions for the time trend + ARIMA models.
##
# Set the forecast horizon
f.h <- t.prime - t

# SEACF Prediction
crash.test$pred.seacf <- (predict(z.model, newdata = crash.test, type = "response") + forecast(ARMA.SEACF.ML, h = f.h)$mean + 1)^2
# AIC ARMA model precitions
crash.test$pred.aic <- (predict(z.model, newdata = crash.test, type = "response") + forecast(ARMA.AIC.ML, h = f.h)$mean + 1)^2

# SARIMA model on the transformed crashes
crash.test$pred.sar <- (forecast(sarima.final, h = f.h)$mean +1)^2


# Auto.arima seasonl model on the transformed crashes
crash.test$pred.asar <- (forecast(a.sarima, h = f.h)$mean +1)^2

# Test naive
crash.test$naive <- crash.train[nrow(crash.train)]$NUM_CRASHES


```


```{r}
# Calculate the MSEP
MSEP <- function(actual, pred)
{
    return(1 / length(actual) * sum((actual - pred)^2))
}
```

```{r}
# Get a number of the MSEP's
# TODO turn this into a table
# naive
round(MSEP(crash.test$NUM_CRASHES, crash.test$naive), digits = 4)

# SEACF
round(MSEP(crash.test$NUM_CRASHES, crash.test$pred.seacf), digits = 4)

# AIC
round(MSEP(crash.test$NUM_CRASHES, crash.test$pred.aic), digits = 4)


# No time series

# Sarima
round(MSEP(crash.test$NUM_CRASHES, crash.test$pred.sar), digits = 4)


# auto SARIMA
round(MSEP(crash.test$NUM_CRASHES, crash.test$pred.asar), digits = 4)
# 
```
Which is the best model by MSPE?
- It is the time trend model fit using AIC

```{r}
# TODO fix the green line
# Compare some of the forecasts via plots
par(mar = c(5, 4, 4, 8), xpd = TRUE)  # extra space on right
plot(ts(crash.test$NUM_CRASHES,start = 912 ), type = "l", lwd = 2, ylim = range(c(crash.test$NUM_CRASHES, crash.test$pred.seacf)), 
     main = "Model Test Prediction vs Actual and Naive Forecasts", ylab = "ACT Daily Road Crashes", bty = "l")
lines(crash.test$pred.seacf, col = "lightblue", lwd = 2, lty = 2) # SEACF Line
lines(crash.test$pred.aic, col = "blue", lwd = 2, lty = 2) # SEACF Line # ETS line 
lines(crash.test$pred.sar, col = "orange", lwd = 2, lty = 2) # aa
lines(crash.test$pred.asar, col = "red2", lwd = 2, lty = 2) # aa
lines(ts(crash.test$naive, start = t+1), col = "green", lwd = 2, lty = 2) # aa

legend("topright", inset = c(-0.35, 0),
       legend = c("Actual", "SEACF", "AIC", "SARIMA", "Auto SARIMA", "Naive "), 
       col = c("black", "lightblue", "blue", "orange", "red2", "green"),
       lwd = 2, 
       lty = c(1, 2, 2, 2, 2, 2),
       bty = "n")

# TODO fix the legend and graph lavels

```
- This graph is super convulated given our time scale, lets just look at the first month

```{r}
# Plot for the first month
par(mar = c(5, 4, 4, 8), xpd = TRUE)  # extra space on right
plot(crash.test[1:28]$NUM_CRASHES, type = "l", lwd = 2, ylim = range(c(crash.test$NUM_CRASHES, crash.test$pred.seacf)), 
     main = "Model Test Prediction vs Actual and Naive Forecasts - First 4 weeks", ylab = "ACT Daily Road Crashes", xlab = "Time", bty = "l")
lines(as.numeric(crash.test[1:28]$pred.seacf), col = "lightblue", lwd = 2, lty = 2) # SEACF Line
lines(as.numeric(crash.test[1:28]$pred.aic), col = "blue", lwd = 2, lty = 2) # SEACF Line # ETS line 
lines(as.numeric(crash.test[1:28]$pred.sar), col = "orange", lwd = 2, lty = 2) # aa
lines(as.numeric(crash.test[1:28]$pred.asar), col = "red2", lwd = 2, lty = 2) # aa
lines(as.numeric(crash.test[1:28]$naive), col = "green", lwd = 2, lty = 2) # aa

legend("topright", inset = c(-0.2, 0),
       legend = c("Actual", "SEACF", "AIC", "SARIMA", "Auto SARIMA", "Naive "), 
       col = c("black", "lightblue", "blue", "orange", "red2", "green"),
       lwd = 2, 
       lty = c(1, 2, 2, 2, 2, 2),
       bty = "n")
```
Which give the best prediction from the figure?
- Looks like the either time time trend SEACF of AIC model give the best fit. The AIC has a slightly better fit only in the first time points. And the two time trend models most accurately reflect the peaks and troughs
- This is the same as the MSPE. Because what does MSPE measure? Average squared distance to true plot. Only wouldn't be the same if there were outlying points skewing

Do the conclusiongs 

Why do some perform well and others not so?
- There are patterns in the data that the SARIMA models can't pick up. Due to changes in calendar etc. the public holidays can't be detected as well. 
- It also appears that the factor in daily time series fit the model better than the structure of the 
- In some ways the performance of the sarima data is impressive, given that I had to find the additional public holiday data.
- Note as well, the SARIMA model only has the daily seasonality, it doesn't consider the months seasonality.
- The difficulty of calendars makes for daily data, monthly seasonality easier with the time trend model
- In both cases, the model with worse fit via diagnostics had worse prediction. This is likely as there was something in our model that the diagnostics is telling us it is missing
- However, comparing between the best time-trend and SARIMA model, the diagnostics can't tell us which of these is better. And the failing time trend model was better than the passing Sarim model
Why do the model diagnostic test fails
- It looks like there was some autoregressive component in the time trend model that the eacf didn't pick up
- Looks like the auto.sarima was missing some ma component that my visual method picked up





# Rolling Window Prediction
- Note that the rolling window still predicts the first point as $y_{T+1}$, so not predicint on the training dataset.
- Seasaonlity is selected on the training set, but estimates come from the window (TODO check this is correct)
- Note because of issues with the number of observations required, I will use floor (0.37) and floor 0.74. This gives us two more evenly spaced samples and lets the seasonality work. 0.37 is the smallest amount of data I can use. Have to use this for all to make them comparable. 0.74 is then used instead of 0.5 to give us a second b that is twice the size as the smallest (like in the question), to hopefully provide a better comparison, as I think the intention was to understand how a larger or smaller window size changed performance

```{r}
# Predict the rolling window for the time trend model
rolling_predict <- function(pdq, # The non-seasonal order
                            type, # either time trend or arima
                            b, # The window size to train the model on
                            PDQ = c(0, 0, 0) # The seasonal order
                            )
    #' @description For a given window and model parameters, predict fit and predict
    #' a windowed forcast
    #' 
    #' @return a numerical list of the T+1 .. T' forecasts based on the previous window 
{
    # Size of T primce
    tp <- t.prime - t
    fcs <- rep(0, tp)
    
    for (i in 1:tp)
    {
        # Range of the window to train on
        range <- (t + i - b - 1):(t + i - 1)
        # Train the estimates but not the order for each model
        if(type == "trend")
        {
            # Specifically train both models on the given range
            model <- lm(z ~ day2 + month + ph *school_holiday, 
                              data = crash[range])
            # Model the residuals as an ARMA process
            arma <- Arima(model$residuals, order = pdq,  method = "ML", include.mean = F)
            # For window get one forecast, then shift forward 1
            f1 <- (predict(model, newdata = crash[t+i], type = "response") + forecast(arma, h = 1)$mean + 1)^2
        } else if(type == "arima")
        {
            # Model the arima model
            arma <- Arima(crash[range]$z, order = pdq,  seasonal = list(order = PDQ, period = 7), 
                          method = "ML",  transform.pars = FALSE, 
                          include.mean = T)
            
            # get the forecast
            f1 <- (forecast(arma, h = 1)$mean + 1)^2
            
        }
    
        
        # Save the windowed forecast
        fcs[i] <- f1
    }
    
    # Finally return the windowed forceast
    return(fcs)
}

# Predict the rolling window for the arima models
```

```{r}
# Get the rolling windows
b.2 <- floor(0.74 * t) # half of the training dataset
b.1 <- floor(0.37 * t) # A quarter of the training dataset

# TODO b.q won't work :(
```

The 
```{r}
# Get the predictions for the different datasets
# SEACF Prediction
crash.test$roll.2.seacf <- rolling_predict(c(0, 0, 1), type = "trend", b = b.2)
crash.test$roll.1.seacf <- rolling_predict(c(0, 0, 1), type = "trend", b = b.1)

# AIC ARMA model predictions
crash.test$roll.2.aic <- rolling_predict(c(1, 0, 0), type = "trend", b = b.2)
crash.test$roll.1.aic <- rolling_predict(c(1, 0, 0), type = "trend", b = b.1)

# SARIMA model on the transformed crashes
crash.test$roll.2.sar <- rolling_predict(c(2, 0, 1), PDQ = c(0, 0, 1), type = "arima", b = b.2) # note reduced P to let it converge
crash.test$roll.1.sar <- rolling_predict(c(2, 0, 1), PDQ = c(0, 0, 1), type = "arima", b = b.1) # note reduced P to let it converge

# Auto.arima seasonal model on the transformed crashes
crash.test$roll.2.asar <- rolling_predict(c(1, 0, 0), PDQ = c(2, 0, 0), type = "arima", b = b.2)
crash.test$roll.1.asar <- rolling_predict(c(1, 0, 0), PDQ = c(2, 0, 0), type = "arima", b = b.1)


# Test naive - last value
crash.test[, roll.naive := shift(NUM_CRASHES, n = 1, type = "lag", fill = crash.train[nrow(crash.train)]$NUM_CRASHES)]
```


```{r}
# Get the windowed forecasts:

# Naive: Note here naive is the previous value
MSEP(crash.test$NUM_CRASHES, crash.test$roll.naive)

# SEACF
MSEP(crash.test$NUM_CRASHES, crash.test$roll.1.seacf)
MSEP(crash.test$NUM_CRASHES, crash.test$roll.2.seacf)

# AIC
MSEP(crash.test$NUM_CRASHES, crash.test$roll.1.aic)
MSEP(crash.test$NUM_CRASHES, crash.test$roll.2.aic)


# No time series

# Sarima
MSEP(crash.test$NUM_CRASHES, crash.test$roll.1.sar)
MSEP(crash.test$NUM_CRASHES, crash.test$roll.2.sar)

# auto SARIMA
MSEP(crash.test$NUM_CRASHES, crash.test$roll.1.asar)
MSEP(crash.test$NUM_CRASHES, crash.test$roll.2.asar)
# 

```

Do the conclusions from model diagnostics in Section 4 support your findings in the prediction
performance of this section? Why or why not?

In addition, try to explain why some model performs well and other models do not perform
well in terms of MSPE1 (e.g., some model passes the model diagnostics while the others do
not, etc).
- Key idea here is that the more data they are trained on, the better the prediction (i think) for the time trend model. Potentially it couldn't learn the patters as well. The seasonal components maybe aren't as good?
- Do I have better than naive before? If so, the arima components can add more to the forecast has after a few truncated they decay quickly to 0


# Limitations
- School holidays don't account for private school differences or 
- Noted monday and tuesday public holiday resulted in large outlier, try and control for this
- Time trend model assumes that the seasonal affects are constant over time - ideally we would use the recent differences 
- Non-linear trends
- Forecast errors
- data required for holidays etc.
- can't do double seasons
- Another one that is in the text somewhere

# Conclusion

# Bibliography

[1] https://www.data.act.gov.au/Transport/ACT-Road-Crash-Data/6jn4-m8rx/about_data
[2] ACT Transport Strategy 2020
[3] https://files.accesscanberra.act.gov.au/legacy/4188/Crash%20reporting.pdf
[4] TODO ref for public holidays
[5] https://actschoolholidays.com.au/past-years/
[6] https://www.education.act.gov.au/public-school-life/term_dates_and_public_holidays/act-school-terms-dates-and-public-holiday-calendar (Contains historical data on the past dates, chatGPT was used to scrap data from earlier years)
[7] https://otexts.com/fpp3/
[8] https://otexts.com/fpp3/arima-r.html

# Archive
daily plotting, didn't work very well.
```{r}
#| label: fig-tsplot
#| fig-cap: "Daily Number of Reported ACT Road Crashes 2015 to 2019"
#| layout-nrow: 2
#| fig-subcap: 
#|  - "First Three Weeks of Time Series"
#|  - "First year of Time Series"
# Plot the entire time series
# Create labels for the first day of jan and july
# TODO work a little more on the formatting here
plot(ts(ts.crash[1:21]),  ylab = "Number of Crashes")
lines(lowess(ts.crash[1:21]), col = 'red', lwd = 2)
legend("topright", legend = c("LOWESS"), 
       col = "red", lwd = 2)


```
7 difference

```{r}
#| label: fig-d7plot
#| figfig-cap: "Change in Daily Number of Reported ACT Road Crashes 2015 to 2019"
# First difference and make another plot
d7.crash <- diff(bc.crash, lag = 7)

# Note the first observation disapears after differencing
diff.dates <- crash$DATE[-c(1:7)]
dates <- format(diff.dates, "%d-%m") %in% c("01-01", "01-07")
date.labels <- diff.dates[dates]
data.axis <- which(dates)
plot(d7.crash, xaxt = "n", ylab = "Number of Crashes")
# TODO fix the start date as the second
axis(1, data.axis, format(date.labels, "%m/%y"), las = 2)
lines(lowess(d7.crash), col = 'red', lwd = 2)
```
364 day difference

```{r}
#| label: fig-dyearplot
#| figfig-cap: "364 Day Differenced Change in Daily Number of Reported ACT Road Crashes 2015 to 2019"
# 364 day difference, 
dyear.crash <- diff(bc.crash, lag = 364)

# Note the first observation disapears after differencing
diff.dates <- crash$DATE#[-c(1:364)]
dates <- format(diff.dates, "%d-%m") %in% c("01-01", "01-07")
date.labels <- diff.dates[dates]
data.axis <- which(dates)
plot(dyear.crash, xaxt = "n", ylab = "Number of Crashes")
# TODO fix the start date as the second
axis(1, data.axis,  format(date.labels, "%m/%y"), las = 2) # Note, this automatically shifts index based on differencing
lines(lowess(dyear.crash), col = 'red', lwd = 2)
legend("topright", legend = c("LOWESS"), 
       col = "red", lwd = 2)
```
 Old prediction method with differencing
 
 ```{r}
# Function for truncated prediction
predict_truncated <- function(fc.model, # The forecasting model
                  time.model, # The time trend model
                  train, # The training dataset
                  test, # the testing dataset
                  horizon = f.h # The forecastin horizon
                  )
{
    #' The predict_truncated function trained forecasting model and
    #' predicts the text horizon steps using a 1 step truncated forecasting
    #' model.
    #' @return Returns the forecast for the next horizon, converted back to the original scale
    start.val <- train[nrow(train)]$z
    # predict the arma part of the model
    # TODO make sure that fc uses 1 step truncated, it should
    e <- forecast(fc.model, h = horizon)$mean
    # Predict z't using the + e zmodel and time series part
    fc <- predict(z.model, type = 'response', newdata = test) + e
    # Now undoing the differencing and the transformation
    z <- diffinv(fc, lag = 1, xi = start.val)[-1] # remove diff first
    # Undo the boxcox by adding 1 and sqauring
    y <- (z + 1)**2
    return(y)
}
```

```{r}
# Note if the time model works, need to use log for time
time.train <- lm(z ~ day + month + 
                     ph + school_holiday, 
                  data = crash.train)
#summary(time.train)

last.train <-  crash.train[nrow(crash.train)]$NUM_CRASHES

crash.train$ttr <- time.train$residuals

aat.zr <- Arima(crash.train$ttr, c(0, 0, 1), include.mean = F)

# auto.arima model prediction
crash.test$pred.aat <- (predict(time.train, newdata = crash.test, type = "response") + forecast(aat.zr, h = f.h)$mean + 1)^2


# Smoothing
ets <- Arima(crash.train$z, c(0, 1, 1))
crash.test$pred.ets <- (forecast(ets, h = f.h)$mean + 1)^2


```